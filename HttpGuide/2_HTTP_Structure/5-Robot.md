# HTTP结构

## Web机器人

Web机器人是能够在无需人为干预的情况下自动进行一些列Web事务处理的软件程序

- 股票图形机器人每隔几分钟就会向股票市场服务器发送HTTP GET请求，用得到的数据构建股市价格趋势图
- Web统计机器人会收集与万维网规模及发展有关的‘统计’信息，他们会在Web上游荡，统计页面的数量，记录每个页面的大小，所用语言及媒体类型
- 搜索引擎机器人会搜集他们所找到的所有文档，以创建搜索数据库
- 比较购物机器人会从在线商店的目录中收集Web页面，构建商品及其价格的数据库

### 爬虫及爬行方式

爬虫是一种机器人，它会递归的对各种信息性Web站点进行遍历

- 从哪儿开始 --- 根集
    > 根集 --- 爬虫开始访问的URL初始集合

- 链接的提取以及相对链接的标准化
    > 提取页面中的所有超链接到待爬行的页面列表中，并将超链接转换成绝对形式

- 机器人需要记住到过何处，避免环路的出现

- 爬到的URL需要具备快速搜索结构，因为这个url列表中的url数量是相当大的，一些技术
    - 树和散列表 --- 使用这些方法记录以访问的URL
    - 有损的存在位图 --- 在位图上标记已访问的URL
    - 检查点 --- 一定要将已访问过的URL列表保存到硬盘上，防止机器人崩溃
    - 分类 --- 对机器人进行分类，各自负责自己的区域

- URL的别名及对URL进行规范化消除别名的影响

- 文件系统连接环路
    > 文件系统中的符号连接会造成特定的潜在环路

- 动态虚拟Web空间
    > 恶意网关动态生产无数的假Web页面，而这些页面中有执行其他假页面，会导致机器人陷入无止境

- 避免循环和重复
    - 规范化URL --- 避免URL别名
    - 广度优先的爬行 --- 广度有限搜索可以更均匀地分配请求，而不是都压在任意一台服务器上去
    - 节流 ---  限制一段时间内机器人从一个Web站点获取的页面数量
    - 限制URL大小 --- 拒绝爬行超过特性长度（1k）的URL
    - URL/站点黑名单 --- 将会导致机器人爬行问题的站点加入黑名单
    - 模式检测 --- 对爬行的URL出现的一定模式的重复进行检测
    - 内容指纹 --- 提取页面一部分内容计算出校验和，如果一个页面提取的校验和被机器人见过，就当作已经爬过该页面了
    - 人工监视 --- 加入机器人的诊断与日志功能

### 机器人的 HTTP

机器人和所有其他HTTP客户端程序并没有什么区别，都遵循HTTP规范中的规则

机器人最好发送基本的首部信息： `User-Agent`/ `From` / `Accept` / `Referer`, 用于服务器识别

机器人需要支持`Host`首部, 用于在虚拟主机的多个站点中区别不同的站点

机器人最好实现条件HTTP请求，用于保证资源的更新

机器人对响应的处理：处理常见的状态码，从实体中查找信息

网站管理者根据`User-Agent`首部确认机器人，并设计一个处理机器人请求的策略

### 拒绝机器人访问标准

如果一个Web站点有robot.txt文件，那么在访问这个Web站点上任意URL之前，机器人都必须获取它并对齐进行处理，确认是否可以对URL进行访问

```text
# this robots.txt

User-Agent: slurp
User-Agent: webcrwler
Disallow: /private

User-Agent: *
Disallow: 
```
该文件允许机器人Slurp和Webcrawler访问除了private子目录下那些文件之外的所有文件，并阻止所有其他机器人访问站点任何内容

机器人排斥标签形式：`<meta name="ROBOTS" content="directive-list"`

`directive-list`：
- noindex --- 不要对页面内容进行处理，忽略文档
- nofollow --- 不要爬行这个页面的任何外连链接
- index --- 可以多页面内容进行索引
- follow --- 可以爬行这个页面的外连链接
- noarchive --- 不应该缓存这个俄页面的本地副本
- all --- 等价于 index,follow
- none --- 等价于 noindex,nofollow